{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gym\n",
    "import imageio\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Observation [ 4.9990320e-01 -9.0856687e+37 -3.7153059e-01  2.6389975e+38]\n",
      "Sample Action 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Observation\", env.observation_space.sample())\n",
    "print(\"Sample Action\", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor([-0.4959], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Policy(s_size, a_size, 16)\n",
    "p(torch.tensor([0.3,0.4,0.5,0.3]).float())\n",
    "p.act(np.array([0.3,0.4,0.5,0.3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    r = np.array([gamma**i * rewards[i] for i in range(len(rewards))])\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r[::-1].cumsum()[::-1]\n",
    "    return torch.tensor((r - r.mean())/r.std(), dtype=torch.float)\n",
    "\n",
    "\n",
    "def reinforce(policy, optimizer, n_training_epsides, max_t, gamma, print_every):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_training_epsides + 1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        net_rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            x, x_dot, theta, theta_dot = state \n",
    "            r1 = (env.x_threshold - abs(x))/env.x_threshold - 0.8\n",
    "            r2 = (env.theta_threshold_radians - abs(theta))/env.theta_threshold_radians - 0.5\n",
    "            net_reward = r1 + r2\n",
    "            rewards.append(reward)\n",
    "            net_rewards.append(net_reward)\n",
    "            if done:\n",
    "                break \n",
    "        rewards_sum = sum(rewards)\n",
    "        scores_deque.append(rewards_sum)\n",
    "        scores.append(rewards_sum)\n",
    "        \n",
    "        # discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        # R = sum([a * b for a, b in zip(discounts, rewards)])\n",
    "        Rs = discount_rewards(net_rewards, gamma)\n",
    "        # policy_loss = []\n",
    "        # for log_prob in saved_log_probs:\n",
    "        #     policy_loss.append(-log_prob * R)\n",
    "        # policy_loss = torch.cat(policy_loss).sum()\n",
    "        # policy_loss = -torch.cat(saved_log_probs).sum() * R\n",
    "        # Rs = torch.tensor([a * b for a, b in zip(discounts, rewards)])\n",
    "        policy_loss = -torch.cat(saved_log_probs) @ Rs\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Epison {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_training_episodes\": 3000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_space\": s_size,\n",
    "    \"action_space\": a_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create policy and place it to the device\n",
    "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epison 100\tAverage Score: 46.24\n",
      "Epison 200\tAverage Score: 74.87\n",
      "Epison 300\tAverage Score: 303.25\n",
      "Epison 400\tAverage Score: 491.05\n",
      "Epison 500\tAverage Score: 500.00\n",
      "Epison 600\tAverage Score: 496.12\n",
      "Epison 700\tAverage Score: 500.00\n",
      "Epison 800\tAverage Score: 500.00\n",
      "Epison 900\tAverage Score: 473.59\n",
      "Epison 1000\tAverage Score: 500.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\29468\\Downloads\\RL\\Reinforcement-learning-with-tensorflow\\contents\\7_Policy_gradient_softmax\\RL_brain_torch_car.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/29468/Downloads/RL/Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain_torch_car.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m scores \u001b[39m=\u001b[39m reinforce(cartpole_policy,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/29468/Downloads/RL/Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain_torch_car.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                    cartpole_optimizer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/29468/Downloads/RL/Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain_torch_car.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                    cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mn_training_episodes\u001b[39;49m\u001b[39m\"\u001b[39;49m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/29468/Downloads/RL/Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain_torch_car.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                    cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mmax_t\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/29468/Downloads/RL/Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain_torch_car.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                    cartpole_hyperparameters[\u001b[39m\"\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m\"\u001b[39;49m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/29468/Downloads/RL/Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain_torch_car.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                    \u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\29468\\Downloads\\RL\\Reinforcement-learning-with-tensorflow\\contents\\7_Policy_gradient_softmax\\RL_brain_torch_car.ipynb Cell 9\u001b[0m in \u001b[0;36mreinforce\u001b[1;34m(policy, optimizer, n_training_epsides, max_t, gamma, print_every)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/29468/Downloads/RL/Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain_torch_car.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m policy_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mcat(saved_log_probs) \u001b[39m@\u001b[39m Rs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/29468/Downloads/RL/Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain_torch_car.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/29468/Downloads/RL/Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain_torch_car.ipynb#X11sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/29468/Downloads/RL/Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain_torch_car.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/29468/Downloads/RL/Reinforcement-learning-with-tensorflow/contents/7_Policy_gradient_softmax/RL_brain_torch_car.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mif\u001b[39;00m i_episode \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\29468\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\29468\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = reinforce(cartpole_policy,\n",
    "                   cartpole_optimizer,\n",
    "                   cartpole_hyperparameters[\"n_training_episodes\"], \n",
    "                   cartpole_hyperparameters[\"max_t\"],\n",
    "                   cartpole_hyperparameters[\"gamma\"], \n",
    "                   100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "841e27f2a14d44ccd92f4291012cb59f687474acc5b2dafa531afbd4ea5b7136"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
